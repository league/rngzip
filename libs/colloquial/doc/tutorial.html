<HTML>
<BODY>

<HEAD>
<TITLE>
Arithmetic Coding
</TITLE>
</HEAD>

<BODY>

<H1>Arithmetic Coding and PPM</H1>

<P>
Arithmetic coding is a general technique for coding the outcome of a
stochastic process based on an adaptive model.  The expected bit rate
is the cross-entropy rate of the model versus the actual process.
PPM, prediction by partial matching, is an adaptive statistical model
of a symbol sequence which models the likelihood of the next byte based
on a (relatively short) suffix of the sequence of previous bytes.


<H2>Byte Stream Models</H2>

<P>
Although arithmetic coding works with any sample space, taking the
outcomes to be bytes allows the coding of text in arbitrary character
sets as well as binary files.  Encoding proceeds a byte at a time and
provides output a bit at a time; decoding proceeds a bit at a time and
provides output a byte at a time. The end-of-file outcome is coded as
a separate outcome, mirroring the behavior of byte reading in both
Java and C streams. 

<P>
Suppose <code>b[0],...,b[m-1]</code> is a sequence of bytes of length
<code>m</code> where <code>b[m-1]=EOF</code>. A suitable model
provides an estimate <code>P(b[n]|b[0],...,b[n-1])</code> for the
probability of the <code>n</code>th byte in the stream or end-of-file
given its observation of the previous <code>n-1</code> bytes.  The
number of bits required to encode <code>b[n]</code> is approximately
<code>-log_2 P_n(b[n]|b[0],...b[n-1])</code>. A slight penalty is
incurred based on the precision of the underlying arithmetic.


<H2>Encoding with Cumulative Probabilities</H2>
<P>
In a sequence of bytes <code>b[0], b[1], ..., b[m-1]</code>, the byte
<code>b[n]</code> is coded based on an ordering <code>b_0, ...,
b_256</code> of the all 256 bytes and the end-of-file symbol.
Assuming <code>b[n]=b_k</code> for some <code>k</code>, the arithmetic
encoder stores the sub-interval

<blockquote>
  <code>[ SUM_j &lt; k P(b[j]|b[0],...,b[j-1]),</code>
  <br>
  <code>&nbsp;  SUM_j &lt;= k P(b[j]|b[0],...,b[j-1]) ]</code>
</blockquote>

of <code>[0.0,1.0]</code>.  Independently of the indexing, the width
of the interval is <code>P(b[n]|b[0],...,b[n-1])</code>.  In practice,
the interval is approximated using three integers <code>low</code>,
<code>high</code>, and <code>total</code>, which define the interval
<code>[low/total,high/total]</code>.  The difference
<code>high-low</code> must be non-zero for every outcome with non-zero
probability. A slight loss in coding precision arises from this
integer approximation and is bounded by the precision induced by
<code>total</code>.


<H2>Decoding with Cumulative Probabilities</H2>
<P>
For decoding, the <code>total</code> used to encode the next symbol's
interval is provided by the model. This requires each possible outcome
<code>P(.|b[0],...,b[n-1])</code> to be coded with
the same <code>total</code> value.  Given the total, the decoder returns a
count <code>k</code> such that <code>low&lt;=k&lt;high</code>. The model
then determines the symbol from the count <code>k</code>.  The coder
encodes byte <code>b[n]</code> based on the previous symbols
<code>b[0],...,b[n-1]</code>, enabling it to update its model after
every symbol. The decoder is able to reconstruct the same model after
decoding each symbol.  The only restriction on the models are that
they are trained only on the previous <code>n-1</code> symbols; in
particular, they cannot use information about the symbol being coded.


<H2>Models</H2>

<P>
The arithmetic coder is distributed with several illustrative models,
culminating in PPMC, prediction by partial matching with model C.
When introduced in the 1980s, PPM provided the best known compression
on general text and binary data such as images. It has since been
beaten by embellishments, first PPMC a few years later which had
better fallback strategies for unseen contexts, then PPM* a decade
after that, which kept track of unbounded length contexts, and then
more recently, PPMZ, which provides several improvements to both
estimation and implementation.

<H3>Uniform Model</H3>
<P>
Uniform distributions assign the same probability to every outcome.
The uniform distribution codes a byte <code>b</code> as the interval
<code>[low,high,total] = [b,b+1,257]</code> and end-of-file as
<code>[256,257,257]</code>.  The expected entropy rate is <code>-log_2
1/257</code>, just over 8 bits.  The loss is due to encoding
end-of-file. For a regular file, the file system keeps track of its
length so that all 256 bytes may be included.  Read and write in
languages like C and Java return bytes in the range 0-255 and return
-1 if the end-of-file is reached; essentially 257 outcomes.  An
alternative implementation of arithmetic coding would store the length
<code>n</code> separately in <code>log_2 n</code> bits. Although this
is typically more efficient than coding end-of-file in a model, it
disrupts the streaming nature of coding/decoding.  

<H3>Adaptive Unigram Model</H3>
<P>
The adaptive unigram model keeps a count of previously seen bytes and
estimates the likelihood of a byte <code>b</code> based on its
frequency <code>count(b)</code>, where each byte's count is
initialized to <code>1</code>.  Adding <code>1</code> to unseen events
in this way is known as Laplace smoothing.  Whenever the total count
exceeds the maximum, the counts are rescaled by dividing by
<code>2</code> and rounding up to at least <code>1</code>.  The model
is adaptive in that it updates after each byte; it is a unigram
because strings of length 1 are used.  A binary search is used during
decoding to determine the interval, but the totals are updated by a
linear traversal of all bytes before the updated byte in the order.  A
more efficient implementation would involve Fenwick trees (Fenwick
1994) or a frequency-balanced model (Moffat 1999).

<H3>PPM: Prediction by Partial Matching</H3>
<P>
The PPM model (Cleary and Witten 1984) predicts the next byte based on
a sequence of previous bytes.  It generalizes the byte frequency model
by keeping counts for sequences of bytes b_1,..,b_n for some value of
n (to be precise, we write PPM(n) for the <code>n</code>th order PPM
model).
<P>
The PPM model illustrates the final aspect of the arithmetic coding
model -- escapes.  A model has the option of informing the coder that
it doesn't currently know how to code the symbol and providing the
likelihood interval for this "escape" outcome.  Then, the coder asks
again.  With PPM, after each escape, the context is narrowed to a
shorter prefix of the current byte, eventually grounding out in the
uniform distribution.  At each escaped context in the backoff, outcomes
that have been seen in longer contexts are excluded from the counts.
<P>
The PPM model is configurable by constructor for maximum context
length, and rescaling and maximum count parameters can be tuned
through constants in the code.

<H2>I/O: BitInput and BitOutput</H2>
<P>
Arithmetic coding relies on processing files a single bit at a
time. The classes <code>BitInput</code> and <code>BitOutput</code>
wrap ordinary input and output streams to provide buffered access to
individual bits.

<H2>Sets of Bytes</H2>
<P>
In order to implement PPM with tight compression, it is necessary to
keep track of which symbols have been seen in escaped contexts so that
their counts may be subtracted from the narrower contexts. The
representation of the bytes that have been seen is handled with a
compact, generic implementation of byte sets.  The members are stored
in the bits four long values and accesses through shifting and
masking. (This was found to be faster and smaller than a binary
array.)


<H2>Other Arithmetic Coders</H2>

<P>
There are several other arithmetic coders available, all written in straight C.

<H3>Witten, Neal and Cleary's</H3>
The original, from Witten, I. H., R. Neal,and J. G. Cleary. 1987. Arithmetic coding for
data compression. <i>Communications of the ACM</i> <b>30</b>(6): 520-540.

<blockquote>
<a href="ftp://ftp.cpsc.ucalgary.ca/pub/projects/ar.cod/cacm-87.shar">ftp://ftp.cpsc.ucalgary.ca/pub/projects/ar.cod/cacm-87.shar</a>
</blockquote>

<H3>Mark Nelson's</H3>
<P>
Mark Nelson's refactoring of the original CACM code
for <a href="http://www.ddj.com/"><I>Dr. Dobbs Journal</I></a>, February 1991:
<blockquote>
<a href="http://dogma.net/markn/articles/arith/part1.htm">http://dogma.net/markn/articles/arith/part1.htm</a>
</blockquote>

<H3>Radford Neal's</H3>
<P>
Radford Neal's low-precision version:
<blockquote>
<a href="ftp://ftp.cs.toronto.edu/pub/radford/lowp_ac.shar">ftp://ftp.cs.toronto.edu/pub/radford/lowp_ac.shar</a>
</blockquote>

<H3>Alistair Moffat's</H3>
<P>
Alistair Moffat's latest, based on
Moffat, Neal, and Witten (1998), along with pointers to older systems:
<blockquote>
<a href="http://www.cs.mu.oz.au/~alistair/arith_coder/">http://www.cs.mu.oz.au/~alistair/arith_coder/</a>
</blockquote>

<H3>Charles Bloom's</H3>
<P>
<a href="http://www.cbloom.com/src/ppmz.html">PPMZ page</a> with
source code and results.  This is currently the best performing coder
in terms of compression and speed, including several improvements to
both estimation itself and to computing adaptive models efficiently.



<H2>Test Corpora</H2>
<P>
These corpora are commonly used to evaluate compression schemes.
<UL>
<LI> <a href="ftp://ftp.cpsc.ucalgary.ca/pub/projects/text.compression.corpus/">The Calgary Corpus</a>.  The original.
<LI> <a href="http://corpus.canterbury.ac.nz/">The Canterbury Corpus</a>.  A larger test suite, including the Calgary corpus.
</UL>


<H2>Benchmark Results</H2>

<P>
<a href="http://compression.ca/">Jeff Gilchrist's Archive Comparison Test</a>. Includes extensive links to existing compressors.


<H2>Compression FAQs</H2>

<P>
<a href="http://www.faqs.org/faqs/compression-faq/part1/preamble.html">Compression FAQ</a>


<H2>Other Tutorials</H2>

<P>
<a href="http://geocities.com/eri32/">Art of Lossless Data Compression</a>



<H2>References for Arithmetic Coding and PPM</H2>
<UL>

<LI>Aberg, J., Y. Sharktov, and B.J.M. Schmeets. 1997. Towards understanding and improving escape probabilities in PPM.
<i>IEEE Data Compression Conference</i>:52-61.

<LI>Aberg, J. and Y. Shtarkov. 1997. Text compression by context tree
weighting. <i>IEEE Data Compression Conference</i>:377-386.

<LI>Aberg, J., Y. M. Shtarkov, and B.J.M. Smeets. 1997.
Multialphabet coding with separate alphabet description. In Proc. of
Compression and Complexity of SEQUENCES 1997, pages 56-65. IEEE
Computer Society.

<LI> Bell, T.C., J.G. Cleary, and I.H. Witten. 1990. <i>Text
compression.</i> Prentice Hall, Englewood Cliffs, NJ.
<BR><small>Introduces Calgary Corpus</small>

<LI>T.C. Bell and I.H. Witten. 1994. Relationship between greedy parsing
and symbolwise text compression. <i>Journal of the ACM</i>, <b>41</b>(4):708--724.
<BR><small>Connection between Lempel-Ziv and statistical coding</small>

<LI>T.C. Bell, I.H. Witten and J.G. Cleary. 1989. Modeling for text
compression. <i>ACM Computing Surveys</i> <b>21</b>(4):557-591.

<LI>J.L. Bentley, D.D. Sleator, R.E. Tarjan, and V.K. Wei. 1986. A
locally adaptive data compression scheme. <i>Communications of the ACM</i>
<b>29</b>(4):320--330.  
<BR><small>Code based on recency statistic using move-to-front lists.</small>

<LI>Bloom, C. 1995.  New techniques in context modeling and arithmetic encoding.
<BR><small>Introduces deferred summation and PPMCB, which involves only keeping
a single outcome for long contexts.</small>

<LI>Bloom, C. 1995. Solving the problems of context modeling.
<BR><small>Introduces PPMZ, containing local-order estimation (LOE) to predict
which order model to use and secondary escape estimation (SEE), which estimates
the escape probabilities dynamically.</small>  

<LI>Bunton, S. 1997. An executable taxonomy of on-line modeling algorithms.
In <i>IEEE Data Compression Conference</i>. 42-51.

<LI>Bunton, S. 1997. <i>On-Line Stochastic Processes in Data Compression.</i>
University of Washington.

<LI>Cheney, J. 2001. Compressing XML with multiplexed hierarchical
models. In <i>IEEE Data Compression Conference</i>, pages
163--172.

<LI>Cleary, J. G. and W.J. Teahan. 1995. Experiments in the zero frequency problem.
<i>IEEE Data Compression Conference</i> 480.

<LI>Cleary, J.G. and I.H. Witten. 1984. Data compression using
adaptive coding and partial string matching. <i>IEEE Transactions on
Communications.</i> <b>32</b>(4):396-402.  
<br><small>Introduces
PPM</small>

<LI>Cleary, J.G., W.J. Teahan, and I.H. Witten. 1995. Unbounded length contexts for PPM.
<i>IEEE Data Compression Conference (DCC)</i>. 

<LI>Cleary, J.G., and W.J. Teahan. 1997. Unbounded context lengths for
PPM. <i>The Computer Journal</i> <b>40</b>(2/3):67-75.
<br><small>Introduces PPM*</small>

<LI>Cormack, G.V. and R.N. Horspool. 1984. Algorithms for adaptive Huffman codes.
<i>Information Processing Letters</i> <b>18</b>(3):159-165.

<LI>Cormack, G.V. and R.N. Horspool. 1987. Data compression using
dynamic Markov modeling. <i>The Computer Journal</i>
<b>30</b>(6):541-550.

<LI> Cover, T. and J. Thomas. 1991. <i>Elements of Information
Theory</i>. John Wiley and Sons, Inc. New York.

<LI> Drinic, M. and D. Kirovski. 2002.
PPMexe: PPM for Compressing Software.
<i>IEEE Data Compression Conference</i>.
     
<LI>Fenwick, P.M. 1994. A new data structure for cumulative frequency
tables. <i>Software Practice and Experience</i> <b>24</b>(3):327-336.

<LI>Howard, P.G. and J.S. Vitter. 1992. Analysis of Arithmetic Coding for
Data Compression. 
<i>Information Processing and Management</i> <b>28</b>(6):749-763.  

<LI>Howard, P.G. and J.S. Vitter. 1992. Practical Implementations of
Arithmetic Coding. In <i>Images and Text Compression</i>.  Kluwer
Academic Publishers.

<LI>Howard, P.G. and J.S. Vitter. 1993. Design and analysis of fast text compression based on quasi-arithmetic coding.
<i>IEEE Data Compression Conference</i>. 98-107.
<br><small>Introduces PPMD.</small></br>

<LI>Howard, P.G. and J.S. Vitter. 1994. Arithmetic Coding for Data Compression. <i>IEEE Data Compression Conference</i> <b>82</b>(6):857-865. 
<br><small>Table-based probability acceleration.</small>

<LI>Jones, C. 1981. An efficient coding system for long source
sequences. <i>IEEE Trans. Information Theory</i>
<b>27</b>(3):280-291.

<LI>Hischberg, D.S. and D.A. Lelewer. 1992. Context modeling for text
compression. In J.A. Storer, <i>Image and Text
Compression</i>. 113-144.

<LI>Langdon, G.G., Jr., and J. Rissanen. 1982. A simple general binary
source code. <i>IEEE Trans. Information Theory</i>
<b>27</b>:800-803.

<LI>Levene, M. and P. Wood. 2002.
XML Structure Compression.  Manuscript, Birkbeck College, University of London.

<LI>Liefke, H. and D. Suciu. 2000. XMILL: An efficient compressor for
XML data. In <i>Proceedings of the ACM SIGMOD International Conference
on Management of Data</i>. 153-164, Dallas, Texas.

<LI>Manning, C. D. and Schuetze, H. 1999. <i>Foundations of
Statistical Natural Language Processing</i>. MIT Press.
<BR>
<small>Great intro to statistical language modeling.</small>

<LI>Moffat, A. 1990. Implementing the PPM data compression
scheme. <i>IEEE Transactions on Communication</i>
<b>38</b>(11):1917-1921.  
<br><small>Introduces PPMC</small>

<LI>Moffat, A. 1990b. Linear time adaptive arithmetic coding. <i>IEEE
Transactions on Information Theory</i> <b>38</b>(11):1917-1921.

<LI> Moffat, A., R.M. Neal, and I.H. Witten. 1998. Arithmetic coding revisited.
<i>ACM Transactions on Information Systems</i> <b>16</b>(3):256-294.

<LI> Moffat. A. 1999. An improved data structure for cumulative probability tables.
<i>Software - Practice and Experience</i> <b>29</b>(7):647-659

<LI>Moffat, A. and A. Turpin. 2002. <i>Compression and
Coding Algorithms</i>.  Kluwer Academic Publishers.

<LI>Pasco, R. 1976. <i>Source Coding Algorithms for Fast Data
Compression</i>. Ph.D. Thesis. Stanford University.

<LI>Pennebaker, W.B. and J.L. Mitchell. 1988. Probability estimation
for the Q-coder. <i>IBM Journal of Research and Development</i>
<b>32</b>(6):737-752.

<LI> Pennebaker, W.B., J.L. Mitchell, G.G. Langdon, Jr., and
R.B. Arps. 1988. An Overview of the Basic Principles of the Q-Coder
Adaptive Binary Arithmetic Coder. <i>IBM J. Res. Develop.</i>
<b>32</b>:717-726.

<LI> Stuiver, L. and A. Moffat. 1998. Piecewise integer mapping for
arithmetic coding.
<i>IEEE Data Compression Conference</i>. 1-10.

<LI> Rissanen, J. 1976. Generalized Kraft Inequality and Arithmetic
Coding. <i>IBM Journal of Research and Development</i> <b>20</b>(3):198-203.
<BR><small>Theory of arithmetic coding.  Could someone please let me know if Rissanen was
the inventor and this is the original reference.</small>

<LI> Rissanen, J. 1984. Universal coding, information, prediction, and esitmation.
<i>IEEE Transactions on Information Theory</i> <b>29</b>(4):629-636.

<LI> Rissanen, J. and G.G. Langdon. 1979. Arithmetic coding. <i>IBM
Journal of Research and Development</i> <b>23</b>(2):149-162.

<LI>Rissanen, J. and K.M. Mohiuddin. 1989. A multiplication-free
multialphabet arithmetic code. <i>IEEE</i> Transactions on
Communications <b>37</b>(2):93-98.

<LI> Rubin, F. 1979. Arithmetic stream coding using fixed precision
registers. <i>IEEE Trans. Information Theory</i> <b>25</b>(6):672-675.

<LI>Schindler, M. 1998. A fast renormalisation for arithmetic
coding. <i>IEEE Data Compression Conference</i> 572.

<LI> Shannon, C. E. 1948. A mathematical theory of communication.
<i>Bell System Technical Journal</i> <b>27</b>:379-423.
<br><small>Introduces the entire field of information theory.  Among other things, shows that the entropy rate of the source stochastic process determines a lower bound on the expected bits per symbol in a coder.</small>

<LI> Shkarin, D. ????. PPM: one step to practicality.  Manuscript.

<LI>Slattery, M.J.  and J.L. Mitchell. The Qx-coder.  1998. IBM Web page.

<LI>Teahan, W. J. and J. G. Cleary. 1997. Models of English text. <i>DCC</i>:12.

<LI>Turpin, A. and A. Moffat. 1997. Efficient approximate adaptive
coding. <i>IEEE Data Compression Conference</i>. 357-366.

<LI>Witten, I.H. and T. Bell. 1991. The zero frequency problem.  <i>IEEE Transactions
on Information Systems</i> <b>37</b>(4):1085.

<LI> Witten, I.H., A. Moffat and T.C. Bell. 1999. <i>Managing
Gigabytes: Compressing and Indexing Documents and Images, Second
Edition</i>. Morgan-Kaufmann Publishers.

<LI> Witten, I.H., R.M. Neal and J.G. Cleary. 1987. Arithmetic coding
for data compression. <i>Communications of the ACM</i> <b>30</b>(6):520-540.
<BR><small>Practice of arithmetic coding for compression</small>

<LI> Witten, I.H., R.M. Neal, and J.G. Cleary. 1988. Compress and
compact discussed further. <i>Communications of the ACM</i> 31(9): 1140-1145.

</UL>

</BODY>

<ADDRESS>
<HR>
<font size=-1>
Copyright 2002. Bob Carpenter.  Maintained by <a href="http://www.colloquial.com/carp">Bob Carpenter</a>,
<a href="mailto:carp@colloquial.com">carp@colloquial.com</a>.
</font>
</ADDRESS>
</HTML>
